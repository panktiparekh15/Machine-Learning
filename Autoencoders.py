# -*- coding: utf-8 -*-
"""hw8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IngQASzKmGez6YB_553ceZ90IFOQ5fUQ

# Homework 8: Coding

**Due Tuesday December 9th, 11:59pm.**

**This is a group assignment.**

**Submit hw8.ipynb link to Gradescope.**

# Question 2: Autoencoders 

In this question, we will try fitting the [Fashion MNIST dataset](https://github.com/zalandoresearch/fashion-mnist) to a vanilla, fully connected autoencoder. You won't have to experiment with the architecture, as the architecture is already given in the PDF of the homework. Please use Google Colab to run this section to maintain consistency across the class. 

Refer to [Pytorch tutorials](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) if you need a refresher on PyTorch.

*Tip: To speed up model training, be sure to go to:"Runtime->Change runtime type->Hardware accelerator: GPU" to use a GPU*
"""

!pip install tqdm # you might need to install tqdm at first

import os
import torch
import torchvision
from torch import nn
from torch.autograd import Variable
from torch.utils.data import DataLoader
from torchvision import transforms
from torchvision.datasets import FashionMNIST
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import numpy as np
from tqdm import tqdm

# Seed set for reproducibility. Do not change this.
torch.manual_seed(15)

"""## 2.1 Constructing the Autoencoder

This section is where you are supposed to code up the architecture and train a simple autoencoder. Helper functions are provided for you.

### Helper Functions to Visualize Images
"""

def convert_to_img(x):
    '''
    Converts the output of the autoencoder network to a batch of images that can be viewed. 
    Essentially reverses the transformation applied to the input dataset. 

    Args:
        x: Output layer of the autoencoder, with the batch size
    Returns:
        x: Viewable image, reshaped to be an array of images.

    '''
    x = 0.5 * (x + 1)
    x = x.clamp(0, 1)
    x = x.view(x.size(0), 1, 28, 28)
    return x

# Making a Grid to print images on screen  
  
def imshow(img,epoch):
    '''
    Converts a torchvision grid to be viewable on matplotlib. 

    Args:
        img: of the type Torchvision.utils.grid(img). 
        epoch: the epoch that this image was recorded on 
    Returns:
        Returns nothing. Plots the Grid.
    '''
    npimg = img.numpy()
    
    figure(num=None, figsize=(8, 6), dpi=150, edgecolor='k')
    plt.axis('off')
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.title("Reconstruction of last minibatch of epoch "+str(epoch))
    
def data_to_model(img):
    '''
    Takes in the image batch given by the dataloader and returns batch that can be fed into the model
    
    Args:
        img: Image batch obtained from the dataloader iterator 
    Returns:
        img: Image batch that can be fed into the model
    '''
    img = img.view(img.size(0), -1)
    img = Variable(img).cuda()
    
    return img

"""### Data pre-processing and Dataloader generation
The given helper code below applies a pre-processing transformation and also generates the iterator through which the data will be sampled from.
"""

# Image transform, normalization 
mean = 0.5 #TODO: Enter value of mean to be used
std = 0.5 #TODO: Enter value of std to be used

img_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((mean,), (std,))
])

#Dataset Generation

batch_size = 128 #TODO: Enter a suitable batch size.

dataset = full_data = FashionMNIST('./data',download=True, transform=img_transform) # Loading the dataset 

dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True) # Dataset that is mainly used for training

full_dataload = iter(DataLoader(full_data,batch_size=len(full_data),shuffle=False)) # Dataset object to get the full dataset returned when iterated.
                                                                                    # This will be used to calculate the reconstruction errors, and to plot latent spaces. 
full_imgs, full_label = full_dataload.next()

'''
This is the autoencoder class where you are supposed to set up the architecture, and the forward pass.
The constructor is setup so that it takes in a variable 
'''

class autoencoder(nn.Module):
    def __init__(self, n=2):
        super(autoencoder, self).__init__()
        '''
        The __init__ takes in 'n' which denotes the size of the bottleneck layer. 
        '''
        ### TODO: Implement the architecture with an encoder layer and a decoder layer 
        #         as defined in the problem set PDF. Be sure to use bias terms here.
        #
        #### Start your code here ####
        self.encoder = nn.Sequential(
                        nn.Linear(28*28, 256, bias = True),
                        nn.ReLU(),
                        nn.Linear(256, 64, bias = True),
                        nn.ReLU(),
                        nn.Linear(64, n, bias = True),
                        nn.ReLU()
        ) # TODO: fill the architecture
        
        self.decoder = nn.Sequential(
                        nn.Linear(n, 64, bias = True),
                        nn.ReLU(),
                        nn.Linear(64, 256, bias = True),
                        nn.ReLU(),
                        nn.Linear(256, 28*28, bias = True),
                        nn.Tanh()
        ) # TODO: fill the architecture
        #### End your code here 
        
    def forward(self, x):
      
        ### TODO: Implement the forward pass, by taking in the input batch of images x, and returning 
        #         the output of the network 
        
        ### Start your code here ###
        x = self.encoder(x)
        x = self.decoder(x)
        ### End your code here ###
        
        return x

'''
You will implement the main training loop here 

'''

def train(num_epochs, dataloader, model, criterion, optimizer):
    '''
    Takes in all necessary parameters to train the model and returns the model and the loss curves.

    Args:
      num_epochs: Number of epochs to train for 
      dataloader: The training dataloader object that was given in the helper code 
      model: The autoencoder model from the class
      criterion: Loss criterion
      Optimizer: Optimizer to be used 
    Returns:
      model: trained model 
      loss_curve: A list of mean epoch losses over the range of epochs
    '''

    #### TODO: In this function, you'll implement the main training loop. 
    device = torch.device('cuda')
    model = model.to(device)

    loss_curve = []
    for epoch in tqdm(range(num_epochs)):
        epoch_loss = 0
        for data in dataloader:
            img, label = data
            #### TODO: Implement the forward pass, the loss calculation, and the optimization processes.
            #          The helper function data_to_model will be useful here.
            #          Calculate the losses and add them to the total epoch loss to find the mean epoch loss 
            #### Start your Code Here
            optimizer.zero_grad()

            img = data_to_model(img)
            output = model(img)

            loss = criterion(output, img.detach())
            epoch_loss = epoch_loss + loss.item()

            loss.backward()
            optimizer.step()
            #### End your code Here

        epoch_loss = epoch_loss / len(dataloader)
        loss_curve.append(epoch_loss)
        # break
        print(f' mean epoch loss: {epoch_loss:.4f}')

        if epoch % 10 == 0: 
            # For every 10 epochs, take the output of the last minibatch of the epoch and print the reconstruction.
            pic = convert_to_img(output.cpu().data)
            imshow(torchvision.utils.make_grid(pic),epoch)

    return model, loss_curve

"""### Main code for Question 2.1
Answer question 2.1 by running this main code. You are asked to plot the training curve of the autoencoder, with **bottleneck layer size being 2**. You are also asked to plot the reconstructions of the autoencoder while it is training at epochs 0, 10, and 20. The optimizer we will use is `torch.optim.Adam()` and the criterion we will use is `nn.MSELoss()`.

The code to train the model
"""

num_epochs = 25 # Run 25 epoches
learning_rate = 1e-3 #TODO: Give in a suitable learning rate for the optimizer. 

print('-------TRAINING WITH HIDDEN LAYER SIZE 2 ------ ')

#TODO: Create a model object of the autoencoder class with bottleneck layer size (n) being 2. 
#      Define the criterion and the optimizer. Call the train function. 

model =  autoencoder(n = 2)#TODO
criterion = nn.MSELoss() #TODO
optimizer = torch.optim.Adam(model.parameters(), learning_rate)  #TODO

trained_model, loss_curve = train(num_epochs, dataloader, model, criterion, optimizer) #TODO

"""The code for plotting the training curve is given below to you."""

epochs = np.arange(num_epochs)
plt.figure()
plt.plot(epochs,loss_curve,label='size 2')
plt.title("Loss vs num Epochs for bottleneck layer size 2")
plt.legend()
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.show()

"""### Latex question 2.1:
Please  report  the  reconstructed  images  from  Epoch  0,  Epoch  10  and  Epoch  20.   These  reconstruc-tions are the output of your autoencoder for the last minibatch of the epoch.  Record these outputs for0, 10, 20 Epochs respectively.  Also report the training curve of the autoencoder (mean epoch loss vsepochs).

## 2.2 Latent Space Decomposition

In this section, you are asked to plot the latent space of the encoded inputs of the model that was trained above in 3.1. You are asked to plot the encoding of the full image set corresponding to the classes 0 and 7. Use the `full_imgs` image batch that was given as a part of the Dataloader helper code.
"""

##### TODO: 
#           Pass the extacted image batches for class labels 5, 7 and 8 
#           (img_5, img_7, img_8) into trained_model trained in Section 3.1, 
#           and extract the *encoding* only (ie. the output of the forward layer). 
#           You will want to use the data_to_model helper function defined above.
#           At the end, have your encoding for classes 5, 7 and 8 as 
#           'enc_5', 'enc_7' and 'enc_8'
#           Hint: To read out a Variable var, use var.detach().cpu().numpy()

img_5 = full_imgs[full_label==5]
img_7 = full_imgs[full_label==7]
img_8 = full_imgs[full_label==8]

### Start your code here ###
img_5 = data_to_model(img_5)
img_7 = data_to_model(img_7)
img_8 = data_to_model(img_8)

with torch.no_grad():
  enc_5 = model.encoder(img_5).detach().cpu().numpy() # TODO: Find this
  enc_7 = model.encoder(img_7).detach().cpu().numpy() # TODO: Find this
  enc_8 = model.encoder(img_8).detach().cpu().numpy() # TODO: Find this

### End your code here ###

"""The plotting code for plotting the encoding of the classes 5, 7 and 8 is given below to you. Use this to generate the plot. """

## DON'T change the color since the color consistency is convenient for grading.
'''
Plotting latent space of encoding of model
'''
plt.figure()
plt.scatter(enc_5[:,0], enc_5[:,1], color='gold', label='5')
plt.scatter(enc_7[:,0], enc_7[:,1], color='teal', label='7')
plt.scatter(enc_8[:,0], enc_8[:,1], color='crimson', label='8')
plt.legend()
plt.xlabel('ENC 0')
plt.ylabel('ENC 1')
plt.title("Latent Space decomposition of three classes of Input Data")
plt.show()

"""### Latex question 2.2:
1) You will now plot the latent space of this autoencoder, which is essentially the encoding that is obtained for any given input. In this problem, the latent space is the output of the ReLU 3, after  the  Fully  connected  layer  3.   Use  your  learned  model  from  the  previous  section,  and  plot  theimages with the class label ‘5’, ‘7’, and ‘8’ in the 2D latent space.

2) Based on the previous image, demonstrate both the difference and similarity among thepatterns of label ‘5’, ‘7’ and ‘8’ in the latent space decomposition plot.  How are they different and/orsimilar?  And Why?  Recall the dataset label are:  T-shirt/top 0, Trouser 1, Pullover 2, Dress 3, Coat4, Sandal 5, Shirt 6, Sneaker 7, Bag 8, and Ankle boot 9.

3) From the latent space plot, explain what the encoding has done to the inputs. How is this effect related to what PCA does? Why is this useful?

## 2.3 Reconstruction error vs Bottleneck Layer:

In this section, you build on top of the code you wrote in section 2.1. Here, we vary the size of the bottleneck layer to see how that affects the reconstructions. Here, we will only observe the reconstruction error corresponding to the entire image set of class 0.

### Reconstruction Error
Fill in the function below that takes in the model and the image batch from the dataloader and computes the reconstruction error (`nn.MSELoss`).
"""

#### TODO: Fill in the method below that takes in the model and the image batch from the dataloader
#          and returns the reconstruction error. The helper function data_to_model will be useful.
#          Hint: To read out a Variable var, use var.detach().cpu().numpy()
 
def reconstruction_error(model,img):
    '''
    Args:
      model: Input model 
      img: Image batch from the dataloader

    Returns:
      err: Reconstruction Error, a scalar quantity.
    '''
    err = 0
    ##### Start your code here #####
    img = data_to_model(img)
    pred = model(img)

    loss = criterion(pred, img)
    err = loss.detach().cpu().numpy()
    #### End your code ###

    return err

"""### Main code for 2.3

Run this main code to generate the plots required to answer question 2.3. Use the full_imgs image batch defined above to generate the reconstruction image batch, which is essentially the images belonging to class '5'.
"""

# Main code

num_epochs = 21 # Run 21 epochs
learning_rate = 1e-3 #TODO: Give a suitable number 

bottleneck_layer_size = [4, 8, 16, 32, 64] #TODO: Insert a list of bottleneck layer sizes to train the network on

rec_err = [] # List of reconstruction errors to be obtained for the multiple sizes
losses = [] # List of training curves to be obtained for the multiple sizes

img_rec = full_imgs[full_label==5] #Obtains the reconstruction image batch.

for size in tqdm(bottleneck_layer_size):
    print(f'\n-------TRAINING WITH HIDDEN LAYER SIZE {size}------ ')
    ##### TODO: Initialize the model with the size of the bottleneck layer, define the criterion, 
    #           the optimizer and call the train method that was defined in section 5.1. Pass the trained
    #           model and the image batch for class 0 to the reconstruction error method, and obtain the 
    #           reconstruction error, and append to the list rec_err. Append the training loss to the list, losses.

    ### Start your code here ####
    model =  autoencoder(size)#TODO
    criterion = nn.MSELoss() #TODO
    optimizer = torch.optim.Adam(model.parameters(), learning_rate)  #TODO

    trained_model, loss_curve = train(num_epochs, dataloader, model, criterion, optimizer) #TODO

    rec_err.append(reconstruction_error(model, img_rec))
    losses.append(loss_curve)
    ### End your code here ####

"""The code for plotting the training curves is provided below:"""

# Learning curves for all bottleneck layers. DON't canage this.

epochs = np.arange(num_epochs)
plt.figure()
count = 0
for curve in losses:
    plt.plot(epochs, curve, label=f'size {str(bottleneck_layer_size[count])}')
    count += 1
plt.title("Loss vs Epochs")
plt.legend()
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.show()

"""The plotting code for plotting the variation of the reconstruction error with the bottleneck layer size:"""

# Reconstruction Error vs Hidden layer size. DON't canage this.

plt.figure()
plt.plot(bottleneck_layer_size,rec_err)
plt.title("Reconstruction error vs size of bottleneck layer")
plt.xlabel("Bottleneck layer size")
plt.ylabel("Reconstruction Error")
plt.show()

"""### Latex question 2.3:

1) In this section, you will extend your code in Part 1 to a variable size bottleneck layer. Use your existing code in Part 1 and run the same for the bottleneck layer sizes $[4,8,16,32,64]$. Record the training curves and the final reconstruction errors for the input images belonging to the class label 0 for each of the sizes. Report the combined training curves (mean epoch loss vs epochs) for all the configurations. Also report the reconstructed images for Epoch 20 for each of the configurations.

2) Plot the mean reconstruction error of the 6 different trained models when the input images belong to the class label 5, with respect to the size of the bottleneck layer. What are you observing? How does what you see relate to PCA? What does this tell you about how you can potentially work with a high-dimensional data-space?

# Turning it in

Submit colab link to gradescope (make sure it's accessible to everyone)
"""